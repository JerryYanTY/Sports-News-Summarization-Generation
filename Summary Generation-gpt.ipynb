{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T14:24:40.352576900Z",
     "start_time": "2024-12-06T14:24:40.347122300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import comb\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_path = \"sport_text\"\n",
    "sum_path = \"sport_sum\"\n",
    "\n",
    "input_files = sorted(os.listdir(text_path))\n",
    "sum_files = sorted(os.listdir(sum_path))\n",
    "\n",
    "assert len(input_files) == len(sum_files), \"Number of files and sum files do not match\"\n",
    "assert all(ins == sums for ins, sums in zip(input_files,sum_files)), \"File name mismatch\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T14:24:41.551711900Z",
     "start_time": "2024-12-06T14:24:41.547990100Z"
    }
   },
   "id": "81370f2365875eee",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input texts: 511\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "sum_texts = []\n",
    "\n",
    "for ins, sums in zip(input_files, sum_files):\n",
    "    with open(os.path.join(text_path, ins), 'r') as f:\n",
    "        input_texts.append(f.read().strip())\n",
    "    with open(os.path.join(sum_path, sums), 'r') as f:\n",
    "        sum_texts.append(f.read().strip())\n",
    "        \n",
    "        \n",
    "print(f\"Number of input texts: {len(input_texts)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T14:24:42.890675600Z",
     "start_time": "2024-12-06T14:24:42.672141300Z"
    }
   },
   "id": "dde3a6798c452e75",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def tokenizing(train, test):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    train_encodings = tokenizer(input_texts, max_length = 512, truncation = True, padding = True, return_tensors = 'pt',padding_side='left')\n",
    "    test_encodings = tokenizer(sum_texts, max_length = 512, truncation = True, padding = True, return_tensors = 'pt',padding_side='left')\n",
    "    return train_encodings, test_encodings\n",
    "\n",
    "train_encodings, test_encodings = tokenizing(input_texts, sum_texts)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T14:24:46.813802700Z",
     "start_time": "2024-12-06T14:24:44.340779200Z"
    }
   },
   "id": "acce94da1b8a8743",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "batch_size = 16\n",
    "\n",
    "input_ids = train_encodings[\"input_ids\"]\n",
    "attention_mask = train_encodings[\"attention_mask\"]\n",
    "labels = test_encodings[\"input_ids\"]\n",
    "\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle= False)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T14:24:47.758457800Z",
     "start_time": "2024-12-06T14:24:47.750409100Z"
    }
   },
   "id": "18559d3d01160bea",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 11.192526817321777\n",
      "Epoch 0, Loss: 9.442776679992676\n",
      "Epoch 0, Loss: 8.761029243469238\n",
      "Epoch 0, Loss: 8.56392765045166\n",
      "Epoch 0, Loss: 8.19342041015625\n",
      "Epoch 0, Loss: 7.882939338684082\n",
      "Epoch 0, Loss: 7.8071818351745605\n",
      "Epoch 0, Loss: 7.595464706420898\n",
      "Epoch 0, Loss: 7.758419990539551\n",
      "Epoch 0, Loss: 7.571887016296387\n",
      "Epoch 0, Loss: 7.6816887855529785\n",
      "Epoch 0, Loss: 7.49912691116333\n",
      "Epoch 0, Loss: 7.377882480621338\n",
      "Epoch 0, Loss: 7.283494472503662\n",
      "Epoch 0, Loss: 7.258186340332031\n",
      "Epoch 0, Loss: 7.354409217834473\n",
      "Epoch 0, Loss: 7.334232330322266\n",
      "Epoch 0, Loss: 7.299045562744141\n",
      "Epoch 0, Loss: 7.357757091522217\n",
      "Epoch 0, Loss: 7.408795356750488\n",
      "Epoch 0, Loss: 7.604079723358154\n",
      "Epoch 0, Loss: 7.4713006019592285\n",
      "Epoch 0, Loss: 7.275476455688477\n",
      "Epoch 0, Loss: 7.353168487548828\n",
      "Epoch 0, Loss: 7.062084674835205\n",
      "Epoch 0, Loss: 7.282834529876709\n",
      "Epoch 0, Loss: 7.337283611297607\n",
      "Epoch 0, Loss: 7.443338394165039\n",
      "Epoch 0, Loss: 7.286825656890869\n",
      "Epoch 0, Loss: 7.08817195892334\n",
      "Epoch 0, Loss: 7.119629859924316\n",
      "Epoch 0, Loss: 7.253739833831787\n",
      "Epoch 1, Loss: 7.212149620056152\n",
      "Epoch 1, Loss: 7.289599418640137\n",
      "Epoch 1, Loss: 7.131042003631592\n",
      "Epoch 1, Loss: 7.210402011871338\n",
      "Epoch 1, Loss: 7.295990467071533\n",
      "Epoch 1, Loss: 7.2544708251953125\n",
      "Epoch 1, Loss: 7.19966983795166\n",
      "Epoch 1, Loss: 7.096141815185547\n",
      "Epoch 1, Loss: 7.222697734832764\n",
      "Epoch 1, Loss: 7.135973930358887\n",
      "Epoch 1, Loss: 7.222339630126953\n",
      "Epoch 1, Loss: 7.100271701812744\n",
      "Epoch 1, Loss: 6.9803314208984375\n",
      "Epoch 1, Loss: 6.967828273773193\n",
      "Epoch 1, Loss: 6.895666122436523\n",
      "Epoch 1, Loss: 6.992063522338867\n",
      "Epoch 1, Loss: 6.972318172454834\n",
      "Epoch 1, Loss: 6.950552940368652\n",
      "Epoch 1, Loss: 6.981650352478027\n",
      "Epoch 1, Loss: 7.047668933868408\n",
      "Epoch 1, Loss: 7.245854377746582\n",
      "Epoch 1, Loss: 7.080414295196533\n",
      "Epoch 1, Loss: 6.961729526519775\n",
      "Epoch 1, Loss: 7.061871528625488\n",
      "Epoch 1, Loss: 6.7617692947387695\n",
      "Epoch 1, Loss: 7.0547194480896\n",
      "Epoch 1, Loss: 6.9709248542785645\n",
      "Epoch 1, Loss: 6.990538597106934\n",
      "Epoch 1, Loss: 6.897092819213867\n",
      "Epoch 1, Loss: 6.764137268066406\n",
      "Epoch 1, Loss: 6.793883323669434\n",
      "Epoch 1, Loss: 6.853851795196533\n",
      "Epoch 2, Loss: 6.87398624420166\n",
      "Epoch 2, Loss: 6.939072132110596\n",
      "Epoch 2, Loss: 6.765559673309326\n",
      "Epoch 2, Loss: 6.914648532867432\n",
      "Epoch 2, Loss: 7.005415916442871\n",
      "Epoch 2, Loss: 6.918833255767822\n",
      "Epoch 2, Loss: 6.947458744049072\n",
      "Epoch 2, Loss: 6.756453514099121\n",
      "Epoch 2, Loss: 6.7914228439331055\n",
      "Epoch 2, Loss: 6.822727203369141\n",
      "Epoch 2, Loss: 6.832509517669678\n",
      "Epoch 2, Loss: 6.825019836425781\n",
      "Epoch 2, Loss: 6.730218887329102\n",
      "Epoch 2, Loss: 6.7335124015808105\n",
      "Epoch 2, Loss: 6.63375186920166\n",
      "Epoch 2, Loss: 6.747654438018799\n",
      "Epoch 2, Loss: 6.733051776885986\n",
      "Epoch 2, Loss: 6.715689182281494\n",
      "Epoch 2, Loss: 6.772428512573242\n",
      "Epoch 2, Loss: 6.840353965759277\n",
      "Epoch 2, Loss: 6.999020576477051\n",
      "Epoch 2, Loss: 6.81050443649292\n",
      "Epoch 2, Loss: 6.7864789962768555\n",
      "Epoch 2, Loss: 6.885871887207031\n",
      "Epoch 2, Loss: 6.569846153259277\n",
      "Epoch 2, Loss: 6.86283540725708\n",
      "Epoch 2, Loss: 6.6443023681640625\n",
      "Epoch 2, Loss: 6.702849388122559\n",
      "Epoch 2, Loss: 6.620992183685303\n",
      "Epoch 2, Loss: 6.571166515350342\n",
      "Epoch 2, Loss: 6.607104301452637\n",
      "Epoch 2, Loss: 6.570872783660889\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AdamW\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T15:13:26.346481400Z",
     "start_time": "2024-12-06T14:25:54.507622800Z"
    }
   },
   "id": "6744879519411ebc",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(dataloader) * 3  # Assuming 3 epochs\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T09:07:40.547043800Z",
     "start_time": "2024-12-06T09:07:39.773755100Z"
    }
   },
   "id": "254ed96017a18010",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Average Loss: 3.5054780021309853\n",
      "Epoch 2/3\n",
      "Average Loss: 3.013164237141609\n",
      "Epoch 3/3\n",
      "Average Loss: 2.941358558833599\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model.train()\n",
    "epochs = 3\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Loss: {avg_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T09:22:54.663538800Z",
     "start_time": "2024-12-06T09:07:46.757378900Z"
    }
   },
   "id": "5ff964a4f5191f15",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./gpt2-sports-summary\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"./gpt2-sports-summary\"\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T09:29:19.425550900Z",
     "start_time": "2024-12-06T09:29:18.629702700Z"
    }
   },
   "id": "2c118d978773fdd7",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Move to device if necessary\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T15:42:50.811099100Z",
     "start_time": "2024-12-06T15:42:49.501408300Z"
    }
   },
   "id": "b9159363c58fedb",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "generated_summaries = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids, _ = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        outputs = model.generate(input_ids, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
    "        decoded = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        generated_summaries.extend(decoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T10:35:03.929908900Z",
     "start_time": "2024-12-06T09:37:27.585902400Z"
    }
   },
   "id": "339258197655165b",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': np.float64(0.6215615793265203), 'rouge2': np.float64(0.5823896609995791), 'rougeL': np.float64(0.39069661554102597), 'rougeLsum': np.float64(0.4795879587686361)}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "results = rouge.compute(predictions=generated_summaries, references=sum_texts)\n",
    "print(\"ROUGE Scores:\", results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-07T03:22:18.631617900Z",
     "start_time": "2024-12-07T03:22:05.871053Z"
    }
   },
   "id": "c0a5d1cd585bc4c0",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It was the first time a sprinter has clocked in at 10.04 seconds at the All-Schools Athletics Championships.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Gout recorded the fourth-fastest under-18 100m time in history on Friday, clocking in at 10.04 seconds at the All-Schools Athletics Championships in Queensland.The run, which came in the heats but was wind-assisted and therefore does not count in official records, was also the fourth-fastest ever by an Australian sprinter of any age.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(device)\n",
    "\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=50,  # Limit the generated text length\n",
    "    num_beams=5,        # Use beam search for better quality\n",
    "    early_stopping=True  # Stop early if the model is confident in its prediction\n",
    ")\n",
    "summary = tokenizer.decode(output_ids[0])\n",
    "print(summary[len(prompt):])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-07T03:42:56.949071200Z",
     "start_time": "2024-12-07T03:42:55.089595200Z"
    }
   },
   "id": "f19786c510508e3",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PRAW from Reddit\n",
    "!pip install praw\n",
    "import praw\n",
    "\n",
    "# Initialize Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"\",  # use user's id\n",
    "    client_secret=\"\",  # use user's secret\n",
    "    user_agent=\"\"  # use user's agent specification\n",
    ")\n",
    "# Fetch posts from the \"sports\" subreddit\n",
    "subreddit = reddit.subreddit(\"sports\")\n",
    "posts = []\n",
    "for post in subreddit.hot(limit=10):  # Adjust the limit as needed\n",
    "    # Fetch top-level comments\n",
    "    comments = []\n",
    "    post.comments.replace_more(limit=0)  # Load all top-level comments\n",
    "    for comment in post.comments.list():\n",
    "        comments.append(comment.body)\n",
    "        if len(comments) >= 5:  # Limit to top 5 comments for brevity\n",
    "            break\n",
    "\n",
    "    posts.append({\n",
    "        \"title\": post.title,\n",
    "        \"selftext\": post.selftext.strip(),\n",
    "        \"url\": post.url,\n",
    "        \"comments\": comments\n",
    "    })\n",
    "\n",
    "# Display the posts\n",
    "for idx, post in enumerate(posts):\n",
    "    print(f\"Post {idx + 1}: {post['title']}\")\n",
    "    if post['selftext']:\n",
    "        print(f\"Content: {post['selftext']}\")\n",
    "    else:\n",
    "        print(\"Content: [No text content available]\")\n",
    "        print(f\"Using comments: {post['comments']}\")\n",
    "    print(f\"URL: {post['url']}\")\n",
    "    print()\n",
    "# Test on Reddit\n",
    "# Summarize Reddit posts\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "def process_reddit_posts(posts):\n",
    "    summarized_posts = []\n",
    "\n",
    "    for post in posts:\n",
    "        # Use content, comments, or title as input\n",
    "        content = post[\"selftext\"]\n",
    "        if not content.strip():  # If no content, use comments\n",
    "            content = ' '.join(post.get(\"comments\", []))\n",
    "        if not content.strip():  # If no comments, use the title\n",
    "            content = post[\"title\"]\n",
    "\n",
    "        # Preprocess the content\n",
    "        output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=50,  # Limit the generated text length\n",
    "        num_beams=5,        # Use beam search for better quality\n",
    "        early_stopping=True  # Stop early if the model is confident in its prediction\n",
    "        )\n",
    "        summary = tokenizer.decode(output_ids[0])\n",
    "\n",
    "        # Store the summarized post\n",
    "        summarized_posts.append({\n",
    "            \"title\": post[\"title\"],\n",
    "            \"summary\": summary,\n",
    "            \"url\": post[\"url\"]\n",
    "        })\n",
    "\n",
    "    return summarized_posts\n",
    "\n",
    "\n",
    "# Summarize the fetched Reddit posts\n",
    "summarized_reddit_posts = process_reddit_posts(posts)\n",
    "\n",
    "# Display the summaries\n",
    "for idx, post in enumerate(summarized_reddit_posts):\n",
    "    print(f\"Post {idx + 1}: {post['title']}\")\n",
    "    print(f\"Summary: {post['summary']}\")\n",
    "    print(f\"URL: {post['url']}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca35f2e455678fcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
