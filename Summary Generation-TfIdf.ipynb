{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tf-idf approach"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7931e99dcd174e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')  # Download punkt tokenizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by removing punctuation, converting to lowercase,\n",
    "    and stripping out any extra whitespace.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return text.lower()\n",
    "\n",
    "def summarize_text(file_path, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Summarizes a text file by identifying the most important sentences using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "    - file_path: Path to the input text file.\n",
    "    - num_sentences: Number of sentences to include in the summary.\n",
    "\n",
    "    Returns:\n",
    "    - A summary containing the most relevant sentences.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(content)\n",
    "\n",
    "    # Preprocess sentences\n",
    "    clean_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "    # Create the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
    "\n",
    "    # Compute sentence scores (sum of TF-IDF values)\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).flatten().tolist()[0]\n",
    "\n",
    "    # Rank sentences by their scores\n",
    "    ranked_sentences = np.argsort(sentence_scores)[::-1]\n",
    "\n",
    "    # Extract top-ranked sentences\n",
    "    summary = [sentences[idx] for idx in ranked_sentences[:num_sentences]]\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"\"  # Replace with path\n",
    "summary = summarize_text(file_path)\n",
    "print(\"Summary:\")\n",
    "print(summary)\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Unzip the uploaded file\n",
    "zip_path = \"/content/bbcnews.zip\"  # Replace with the actual uploaded file name if different\n",
    "extract_path = \"/bbcnews\"  # Path to extract the files\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# Navigate to the desired folders\n",
    "articles_path = os.path.join(extract_path, \"BBC News Summary\", \"News Articles\")\n",
    "summaries_path = os.path.join(extract_path, \"BBC News Summary\", \"Summaries\")\n",
    "\n",
    "# Verify the folders and files\n",
    "print(\"News Articles Folders:\", os.listdir(articles_path))\n",
    "print(\"Summaries Folders:\", os.listdir(summaries_path))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f6f00df876e7c7a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define paths\n",
    "articles_path = \"/bbcnews/BBC News Summary/News Articles/sport\"  # Folder containing the articles\n",
    "summaries_path = \"/bbcnews/BBC News Summary/Summaries/sport\"  # Folder containing the corresponding summaries\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, convert to lowercase, and tokenize\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load data\n",
    "articles = []\n",
    "summaries = []\n",
    "file_ids = []\n",
    "\n",
    "for filename in sorted(os.listdir(articles_path)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_id = filename.split(\".\")[0]\n",
    "        file_ids.append(file_id)\n",
    "\n",
    "        # Read and preprocess articles\n",
    "        with open(os.path.join(articles_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            articles.append(preprocess_text(file.read()))\n",
    "\n",
    "        # Read and preprocess summaries\n",
    "        with open(os.path.join(summaries_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            summaries.append(preprocess_text(file.read()))\n",
    "\n",
    "# Create TF-IDF representations\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles)\n",
    "\n",
    "# Extract important sentences for summaries\n",
    "def extract_summary(article, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(article)  # Split into sentences\n",
    "    sentence_vectors = tfidf_vectorizer.transform(sentences)\n",
    "    # Compute similarity scores between sentences and article\n",
    "    similarity_scores = cosine_similarity(sentence_vectors, tfidf_matrix)\n",
    "    # Sort sentences by importance\n",
    "    top_indices = np.argsort(similarity_scores.flatten())[::-1][:num_sentences]\n",
    "    return ' '.join([sentences[i] for i in sorted(top_indices)])\n",
    "\n",
    "# Train a simple regression model to map TF-IDF features to summary vectors\n",
    "summary_tfidf = tfidf_vectorizer.transform(summaries)\n",
    "model = LinearRegression()\n",
    "model.fit(tfidf_matrix, summary_tfidf.toarray())\n",
    "\n",
    "# Test the model on preprocessed Reddit post content\n",
    "def summarize_reddit_post(content):\n",
    "    # TF-IDF representation of the preprocessed content\n",
    "    post_tfidf = tfidf_vectorizer.transform([content])\n",
    "    predicted_summary_vector = model.predict(post_tfidf)\n",
    "\n",
    "    # Tokenize the content into sentences\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    if len(sentences) == 0:  # If content is empty or has no sentences\n",
    "        return \"No summary could be generated.\"\n",
    "\n",
    "    # Compute similarity scores between sentences and predicted summary vector\n",
    "    sentence_vectors = tfidf_vectorizer.transform(sentences)\n",
    "    similarity_scores = cosine_similarity(sentence_vectors, predicted_summary_vector)\n",
    "\n",
    "    # Select the top 3 most relevant sentences\n",
    "    top_indices = np.argsort(similarity_scores.flatten())[::-1][:3]\n",
    "    return ' '.join([sentences[i] for i in sorted(top_indices)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "909580fbc9607bdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PRAW from Reddit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86c7bb502a63e838"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install praw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8bc33f99a66cf8f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# Initialize Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"\",           # use user's id\n",
    "    client_secret=\"\",       # use user's secret\n",
    "    user_agent=\"\"           # use user's agent specification\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b88e568f5658749"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fetch posts from the \"sports\" subreddit\n",
    "subreddit = reddit.subreddit(\"sports\")\n",
    "posts = []\n",
    "for post in subreddit.hot(limit=10):  # Adjust the limit as needed\n",
    "    # Fetch top-level comments\n",
    "    comments = []\n",
    "    post.comments.replace_more(limit=0)  # Load all top-level comments\n",
    "    for comment in post.comments.list():\n",
    "        comments.append(comment.body)\n",
    "        if len(comments) >= 5:  # Limit to top 5 comments for brevity\n",
    "            break\n",
    "\n",
    "    posts.append({\n",
    "        \"title\": post.title,\n",
    "        \"selftext\": post.selftext.strip(),\n",
    "        \"url\": post.url,\n",
    "        \"comments\": comments\n",
    "    })\n",
    "\n",
    "# Display the posts\n",
    "for idx, post in enumerate(posts):\n",
    "    print(f\"Post {idx + 1}: {post['title']}\")\n",
    "    if post['selftext']:\n",
    "        print(f\"Content: {post['selftext']}\")\n",
    "    else:\n",
    "        print(\"Content: [No text content available]\")\n",
    "        print(f\"Using comments: {post['comments']}\")\n",
    "    print(f\"URL: {post['url']}\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cf465477186c51c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test on Reddit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "366ab1ad48d927a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Summarize Reddit posts\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "def process_reddit_posts(posts):\n",
    "    summarized_posts = []\n",
    "\n",
    "    for post in posts:\n",
    "        # Use content, comments, or title as input\n",
    "        content = post[\"selftext\"]\n",
    "        if not content.strip():  # If no content, use comments\n",
    "            content = ' '.join(post.get(\"comments\", []))\n",
    "        if not content.strip():  # If no comments, use the title\n",
    "            content = post[\"title\"]\n",
    "\n",
    "        # Preprocess the content\n",
    "        preprocessed_content = preprocess_text(content)\n",
    "\n",
    "        # Summarize using the trained model\n",
    "        summary = summarize_reddit_post(preprocessed_content)\n",
    "\n",
    "        # Store the summarized post\n",
    "        summarized_posts.append({\n",
    "            \"title\": post[\"title\"],\n",
    "            \"summary\": summary,\n",
    "            \"url\": post[\"url\"]\n",
    "        })\n",
    "\n",
    "    return summarized_posts\n",
    "\n",
    "# Summarize the fetched Reddit posts\n",
    "summarized_reddit_posts = process_reddit_posts(posts)\n",
    "\n",
    "# Display the summaries\n",
    "for idx, post in enumerate(summarized_reddit_posts):\n",
    "    print(f\"Post {idx + 1}: {post['title']}\")\n",
    "    print(f\"Summary: {post['summary']}\")\n",
    "    print(f\"URL: {post['url']}\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc305efdecb4c913"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
